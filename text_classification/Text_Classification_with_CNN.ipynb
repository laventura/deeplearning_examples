{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification with CNN\n",
    "\n",
    "Acknowledgments: Denny Bretz's excellent tutorial here: http://www.wildml.com/2015/12/implementing-a-cnn-for-text-classification-in-tensorflow/\n",
    "\n",
    "**Dataset**\n",
    "- We will use the **Movie Review** dataset. \n",
    "- http://www.cs.cornell.edu/people/pabo/movie-review-data/ \n",
    "\n",
    "The dataset contains 10,662 example review sentences, half positive and half negative. The dataset has a vocabulary of size around 20k. Note that since this data set is pretty small we’re likely to overfit with a powerful model. Also, the dataset doesn’t come with an official train/test split, so we simply use 10% of the data as a dev set. The original paper reported results for 10-fold cross-validation on the data.\n",
    "\n",
    "\n",
    "The approach (from Denny's blog):\n",
    "\n",
    "1. Load positive and negative sentences from the raw data files.\n",
    "2. Clean the text data using the same code as the original paper.\n",
    "3. Pad each sentence to the maximum sentence length, which turns out to be 59. We append special <PAD> tokens to all other sentences to make them 59 words. Padding sentences to the same length is useful because it allows us to efficiently batch our data since each example in a batch must be of the same length.\n",
    "4. Build a vocabulary index and map each word to an integer between 0 and 18,765 (the vocabulary size). Each sentence becomes a vector of integers.\n",
    "\n",
    "\n",
    "**NOTE**  Most of the code is from Denny's original files.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version 0.11.0rc2\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from tensorflow.contrib import learn\n",
    "\n",
    "print('TensorFlow version', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Atul Acharya \n",
      "last updated: 2016-12-06 \n",
      "\n",
      "CPython 3.5.2\n",
      "IPython 5.0.0\n",
      "\n",
      "numpy 1.11.2\n",
      "tensorflow 0.11.0rc2\n",
      "matplotlib 1.5.1\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline \n",
    "\n",
    "%load_ext watermark\n",
    "\n",
    "%watermark -a 'Atul Acharya' -u -d -v -p numpy,tensorflow,matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## First, define a few helper funcs to prepare data\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "import itertools\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def clean_str(string):\n",
    "    \"\"\"\n",
    "    Tokenization/string cleaning for all datasets except for SST.\n",
    "    Original taken from https://github.com/yoonkim/CNN_sentence/blob/master/process_data.py\n",
    "    \"\"\"\n",
    "    string = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", string)\n",
    "    string = re.sub(r\"\\'s\", \" \\'s\", string)\n",
    "    string = re.sub(r\"\\'ve\", \" \\'ve\", string)\n",
    "    string = re.sub(r\"n\\'t\", \" n\\'t\", string)\n",
    "    string = re.sub(r\"\\'re\", \" \\'re\", string)\n",
    "    string = re.sub(r\"\\'d\", \" \\'d\", string)\n",
    "    string = re.sub(r\"\\'ll\", \" \\'ll\", string)\n",
    "    string = re.sub(r\",\", \" , \", string)\n",
    "    string = re.sub(r\"!\", \" ! \", string)\n",
    "    string = re.sub(r\"\\(\", \" \\( \", string)\n",
    "    string = re.sub(r\"\\)\", \" \\) \", string)\n",
    "    string = re.sub(r\"\\?\", \" \\? \", string)\n",
    "    string = re.sub(r\"\\s{2,}\", \" \", string)\n",
    "    return string.strip().lower()\n",
    "\n",
    "\n",
    "def load_data_and_labels(positive_data_file, negative_data_file):\n",
    "    \"\"\"\n",
    "    Loads Movie Review polarity data from files, splits the data into words and generates labels.\n",
    "    Returns split sentences and labels.\n",
    "    \"\"\"\n",
    "    # Load data from files\n",
    "    positive_examples = list(open(positive_data_file, \"r\").readlines())\n",
    "    positive_examples = [s.strip() for s in positive_examples]\n",
    "    negative_examples = list(open(negative_data_file, \"r\").readlines())\n",
    "    negative_examples = [s.strip() for s in negative_examples]\n",
    "    # Split by words\n",
    "    x_text = positive_examples + negative_examples\n",
    "    x_text = [clean_str(sent) for sent in x_text]\n",
    "    # Generate labels\n",
    "    positive_labels = [[0, 1] for _ in positive_examples]\n",
    "    negative_labels = [[1, 0] for _ in negative_examples]\n",
    "    y = np.concatenate([positive_labels, negative_labels], 0)\n",
    "    return [x_text, y]\n",
    "\n",
    "\n",
    "def batch_iter(data, batch_size, num_epochs, shuffle=True):\n",
    "    \"\"\"\n",
    "    Generates a batch iterator for a dataset.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    data_size = len(data)\n",
    "    num_batches_per_epoch = int(len(data)/batch_size) + 1\n",
    "    for epoch in range(num_epochs):\n",
    "        # Shuffle the data at each epoch\n",
    "        if shuffle:\n",
    "            shuffle_indices = np.random.permutation(np.arange(data_size))\n",
    "            shuffled_data = data[shuffle_indices]\n",
    "        else:\n",
    "            shuffled_data = data\n",
    "        for batch_num in range(num_batches_per_epoch):\n",
    "            start_index = batch_num * batch_size\n",
    "            end_index = min((batch_num + 1) * batch_size, data_size)\n",
    "            yield shuffled_data[start_index:end_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the CNN Model\n",
    "\n",
    "The model looks similar to the one in the original paper.\n",
    "http://d3kbpzbmcynnmx.cloudfront.net/wp-content/uploads/2015/11/Screen-Shot-2015-11-06-at-8.03.47-AM.png\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"\n",
    "    A ConvNet for text classification\n",
    "    Uses an embedding layer, following by a convolutional layer, then a max pool, and softmax\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, sequence_length, num_classes, vocab_size,\n",
    "        embedding_size, filter_sizes, num_filters, l2_reg_lambda=0.0):\n",
    "        \n",
    "        ## placeholder for input / output\n",
    "        # X: [None, Sequence Length]\n",
    "        self.input_x = tf.placeholder(tf.int32, [None, sequence_length], name='input_x')\n",
    "        # Y: [None, Num Classes]\n",
    "        self.input_y = tf.placeholder(tf.float32, [None, num_classes], name='input_y')\n",
    "        self.dropout_keep_prob = tf.placeholder(tf.float32, name='dropout_keep_prob')\n",
    "        \n",
    "        # keep track of L2 regularization loss\n",
    "        l2_loss = tf.constant(0.0)\n",
    "        \n",
    "        ## Embedding layer\n",
    "        with tf.device('/cpu:0'), tf.name_scope('embedding'):\n",
    "            # W: embedding matrix - we will learn this during training\n",
    "            W = tf.Variable(tf.random_uniform([vocab_size, embedding_size],\n",
    "                                             -1.0, 1.0),\n",
    "                           name=\"W\")\n",
    "            \n",
    "            self.embedded_chars = tf.nn.embedding_lookup(W, self.input_x)\n",
    "            self.embedded_chars_expanded = tf.expand_dims(self.embedded_chars, -1)\n",
    "            # Result of embedding op:\n",
    "            # a 3D tensor of shape: [None, Sequence length, Embedding size]\n",
    "            \n",
    "        # TF's Conv2D expects a 4d tensor \n",
    "        ''' TensorFlow’s convolutional conv2d operation expects a 4-dimensional tensor with dimensions \n",
    "        corresponding to batch, width, height and channel. \n",
    "        The result of our embedding doesn’t contain the channel dimension, \n",
    "        so we add it manually, leaving us with a layer of shape \n",
    "        [None, sequence_length, embedding_size, 1].\n",
    "\n",
    "        '''\n",
    "        ## Create a conv + max pool layer for each filter size\n",
    "        ''' Note that filter_sizes will usually be [3,4,5] meaning filters will operate\n",
    "            on 3-words, 4-words, 5-words at a time.\n",
    "            Total num filters = num_filters * len(filter_sizes)\n",
    "        '''\n",
    "        pooled_outputs = []\n",
    "        for i, filter_size in enumerate(filter_sizes):\n",
    "            with tf.name_scope(\"conv_maxpool_%s\" % filter_size):\n",
    "                # Conv layer\n",
    "                filter_shape = [filter_size, embedding_size, 1, num_filters]\n",
    "                W = tf.Variable(tf.truncated_normal(filter_shape, stddev=0.1), name=\"W\")\n",
    "                b = tf.Variable(tf.constant(0.1, shape=[num_filters]), name=\"b\")\n",
    "                conv = tf.nn.conv2d(\n",
    "                        self.embedded_chars_expanded,\n",
    "                        W,\n",
    "                        strides=[1,1,1,1],\n",
    "                        padding='VALID',   # narrow conv, no padding\n",
    "                        name='conv')\n",
    "                # >> output shape: [1, sequence_length - filter_size + 1, 1, 1]\n",
    "                # Apply non-linearity\n",
    "                h = tf.nn.relu(tf.nn.bias_add(conv, b), name='relu')\n",
    "                # Maxpool over the outputs\n",
    "                pooled = tf.nn.max_pool(\n",
    "                        h,\n",
    "                        ksize=[1, sequence_length - filter_size + 1, 1, 1],\n",
    "                        strides=[1,1,1,1],\n",
    "                        padding='VALID',\n",
    "                        name='pool')\n",
    "                # >> output shape: [batch_size, 1, 1, num_filters]\n",
    "                \n",
    "                pooled_outputs.append(pooled)\n",
    "                \n",
    "        # Combine all the pooled features\n",
    "        num_filters_total = num_filters * len(filter_sizes)\n",
    "        self.h_pool = tf.concat(3, pooled_outputs)\n",
    "        self.h_pool_flat = tf.reshape(self.h_pool, [-1, num_filters_total])\n",
    "        # >> output shape: [batch_size, num_filters_total]\n",
    "        \n",
    "        ## Dropout\n",
    "        with tf.name_scope(\"dropout\"):\n",
    "            self.h_drop = tf.nn.dropout(self.h_pool_flat, self.dropout_keep_prob)\n",
    "            \n",
    "        ## Final (unnormalized) scores and predictions\n",
    "        with tf.name_scope('output'):\n",
    "            W = tf.get_variable(\"W\",\n",
    "                               shape=[num_filters_total, num_classes],\n",
    "                               initializer=tf.contrib.layers.xavier_initializer())\n",
    "            b = tf.Variable(tf.constant(0.1, shape=[num_classes]), name=\"b\")\n",
    "            l2_loss += tf.nn.l2_loss(W)\n",
    "            l2_loss += tf.nn.l2_loss(b)\n",
    "            \n",
    "            self.scores = tf.nn.xw_plus_b(self.h_drop, W, b, name=\"scores\")\n",
    "            self.predictions = tf.argmax(self.scores, 1, name=\"predictions\")\n",
    "            \n",
    "        # Calc mean cross-entropy loss\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            losses = tf.nn.softmax_cross_entropy_with_logits(self.scores,\n",
    "                                                            self.input_y)\n",
    "            self.loss = tf.reduce_mean(losses) + l2_reg_lambda * l2_loss\n",
    "            \n",
    "        ## Accuracy\n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            correct_predictions = tf.equal(self.predictions,\n",
    "                                          tf.argmax(self.input_y, 1))\n",
    "            self.accuracy = tf.reduce_mean(tf.cast(correct_predictions,\n",
    "                                                  tf.float32),\n",
    "                                          name=\"accuracy\")\n",
    "            \n",
    "        # Add an op to initialize the variables.\n",
    "        init_op = tf.initialize_all_variables()\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Now let's train the CNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "### Define the TF flags\n",
    "\n",
    "# Parameters\n",
    "# ==================================================\n",
    "\n",
    "# Data loading params\n",
    "tf.flags.DEFINE_float(\"dev_sample_percentage\", 0.1, \"Percentage of the training data to use for validation\")\n",
    "tf.flags.DEFINE_string(\"positive_data_file\", \"./data/rt-polaritydata/rt-polarity.pos\", \"Data source for the positive data.\")\n",
    "tf.flags.DEFINE_string(\"negative_data_file\", \"./data/rt-polaritydata/rt-polarity.neg\", \"Data source for the positive data.\")\n",
    "\n",
    "# Model Hyperparameters\n",
    "tf.flags.DEFINE_integer(\"embedding_dim\", 128, \"Dimensionality of character embedding (default: 128)\")\n",
    "tf.flags.DEFINE_string(\"filter_sizes\", \"3,4,5\", \"Comma-separated filter sizes (default: '3,4,5')\")\n",
    "tf.flags.DEFINE_integer(\"num_filters\", 128, \"Number of filters per filter size (default: 128)\")\n",
    "tf.flags.DEFINE_float(\"dropout_keep_prob\", 0.5, \"Dropout keep probability (default: 0.5)\")\n",
    "tf.flags.DEFINE_float(\"l2_reg_lambda\", 0.1, \"L2 regularizaion lambda (default: 0.0)\")\n",
    "\n",
    "# Training parameters\n",
    "tf.flags.DEFINE_integer(\"batch_size\", 64, \"Batch Size (default: 64)\")\n",
    "tf.flags.DEFINE_integer(\"num_epochs\", 50, \"Number of training epochs (default: 200)\")\n",
    "tf.flags.DEFINE_integer(\"evaluate_every\", 100, \"Evaluate model on dev set after this many steps (default: 100)\")\n",
    "tf.flags.DEFINE_integer(\"checkpoint_every\", 100, \"Save model after this many steps (default: 100)\")\n",
    "# Misc Parameters\n",
    "tf.flags.DEFINE_boolean(\"allow_soft_placement\", True, \"Allow device soft device placement\")\n",
    "tf.flags.DEFINE_boolean(\"log_device_placement\", False, \"Log placement of ops on devices\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Parameters:\n",
      "ALLOW_SOFT_PLACEMENT=True\n",
      "BATCH_SIZE=64\n",
      "CHECKPOINT_EVERY=100\n",
      "DEV_SAMPLE_PERCENTAGE=0.1\n",
      "DROPOUT_KEEP_PROB=0.5\n",
      "EMBEDDING_DIM=128\n",
      "EVALUATE_EVERY=100\n",
      "FILTER_SIZES=3,4,5\n",
      "L2_REG_LAMBDA=0.1\n",
      "LOG_DEVICE_PLACEMENT=False\n",
      "NEGATIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.neg\n",
      "NUM_EPOCHS=50\n",
      "NUM_FILTERS=128\n",
      "POSITIVE_DATA_FILE=./data/rt-polaritydata/rt-polarity.pos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FLAGS = tf.flags.FLAGS\n",
    "FLAGS._parse_flags()\n",
    "print(\"\\nParameters:\")\n",
    "for attr, value in sorted(FLAGS.__flags.items()):\n",
    "    print(\"{}={}\".format(attr.upper(), value))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "X shape: (10662, 56)\n",
      "Vocabulary Size: 18758\n",
      "Train/Dev split: 9596/1066\n",
      "x_train: (9596, 56)\n",
      "y_train: (9596, 2)\n",
      "x_dev  : (1066, 56)\n",
      "y_dev  : (1066, 2)\n"
     ]
    }
   ],
   "source": [
    "## Load data\n",
    "print('Loading data...')\n",
    "x_text, y = load_data_and_labels(FLAGS.positive_data_file, FLAGS.negative_data_file)\n",
    "\n",
    "# Build vocabulary\n",
    "max_document_length = max([len(x.split(\" \")) for x in x_text])\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor(max_document_length)\n",
    "x = np.array(list(vocab_processor.fit_transform(x_text)))\n",
    "print('X shape:', x.shape)\n",
    "\n",
    "## for cross -validation - \n",
    "# shuffle\n",
    "np.random.seed(10)\n",
    "shuffle_indices = np.random.permutation(np.arange(len(y)))\n",
    "x_shuffled = x[shuffle_indices]\n",
    "y_shuffled = y[shuffle_indices]\n",
    "\n",
    "# Split train/test set\n",
    "# TODO: This is very crude, should use cross-validation\n",
    "dev_sample_index = -1 * int(FLAGS.dev_sample_percentage * float(len(y)))\n",
    "x_train, x_dev = x_shuffled[:dev_sample_index], x_shuffled[dev_sample_index:]\n",
    "y_train, y_dev = y_shuffled[:dev_sample_index], y_shuffled[dev_sample_index:]\n",
    "print(\"Vocabulary Size: {:d}\".format(len(vocab_processor.vocabulary_)))\n",
    "print(\"Train/Dev split: {:d}/{:d}\".format(len(y_train), len(y_dev)))\n",
    "\n",
    "print('x_train:', x_train.shape)\n",
    "print('y_train:', y_train.shape)\n",
    "print('x_dev  :', x_dev.shape)\n",
    "print('y_dev  :', y_dev.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-700\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-900\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-1000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-1100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-1200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-1300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-1400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-1500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-1600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-1700\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-1800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-1900\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-2000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-2100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-2200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-2300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-2400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-2500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-2600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-2700\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-2800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-2900\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-3000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-3100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-3200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-3300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-3400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-3500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-3600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-3700\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-3800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-3900\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-4000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-4100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-4200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-4300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-4400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-4500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-4600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-4700\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-4800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-4900\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-5000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-5100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-5200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-5300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-5400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-5500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-5600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-5700\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-5800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-5900\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-6000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-6100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-6200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-6300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-6400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-6500\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-6600\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-6700\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-6800\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-6900\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-7000\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-7100\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-7200\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-7300\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-7400\n",
      "\n",
      "\n",
      "Evaluation:\n",
      "\n",
      "Saved model checkpoint to /Users/aa/Developer/DeepLearningExamples/text_classification/runs/1481089620/checkpoints/model-7500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "### Begin training\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "        allow_soft_placement=FLAGS.allow_soft_placement,\n",
    "        log_device_placement=FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    \n",
    "    with sess.as_default():\n",
    "        cnn = TextCNN(sequence_length=x_train.shape[1],\n",
    "                      num_classes=y_train.shape[1],   # 2 output classes in this case\n",
    "                      vocab_size=len(vocab_processor.vocabulary_),\n",
    "                      embedding_size=FLAGS.embedding_dim,\n",
    "                      filter_sizes=list(map(int, FLAGS.filter_sizes.split(\",\"))),\n",
    "                      num_filters=FLAGS.num_filters,\n",
    "                      l2_reg_lambda=FLAGS.l2_reg_lambda)\n",
    "        \n",
    "        # Training procedure\n",
    "        global_step    = tf.Variable(0, name=\"global_step\", trainable=False)\n",
    "        optimizer      = tf.train.AdamOptimizer(1e-3)\n",
    "        grads_and_vars = optimizer.compute_gradients(cnn.loss)\n",
    "        train_op       = optimizer.apply_gradients(grads_and_vars, global_step=global_step)\n",
    "        \n",
    "        # keep track of gradients and sparsity\n",
    "        grad_summaries = []\n",
    "        for g, v in grads_and_vars:\n",
    "            if g is not None:\n",
    "                grad_hist_summary = tf.histogram_summary(\"{}/grad/hist\".format(v.name), g)\n",
    "                sparsity_summary  = tf.scalar_summary(\"{}/grad/sparsity\".format(v.name), tf.nn.zero_fraction(g))\n",
    "                grad_summaries.append(grad_hist_summary)\n",
    "                grad_summaries.append(sparsity_summary)\n",
    "        grad_summaries_merged = tf.merge_summary(grad_summaries)\n",
    "        \n",
    "        # output dir for models and summaries\n",
    "        timestamp = str(int(time.time()))\n",
    "        out_dir = os.path.abspath(os.path.join(os.path.curdir, \"runs\", timestamp))\n",
    "        print(\"Writing to {}\\n\".format(out_dir))\n",
    "        \n",
    "        # Summaries for loss and accuracy\n",
    "        loss_summary = tf.scalar_summary(\"loss\", cnn.loss)\n",
    "        acc_summary  = tf.scalar_summary(\"accuracy\", cnn.accuracy)\n",
    "        \n",
    "        # Train Summaries - save \n",
    "        train_summary_op     = tf.merge_summary([loss_summary, acc_summary, grad_summaries_merged])\n",
    "        train_summary_dir    = os.path.join(out_dir, \"summaries\", \"train\")\n",
    "        train_summary_writer = tf.train.SummaryWriter(train_summary_dir, sess.graph)\n",
    "        \n",
    "        # Dev summaries - save\n",
    "        dev_summary_op     = tf.merge_summary([loss_summary, acc_summary])\n",
    "        dev_summary_dir    = os.path.join(out_dir, \"summaries\", \"dev\")\n",
    "        dev_summary_writer = tf.train.SummaryWriter(dev_summary_dir, sess.graph)\n",
    "        \n",
    "        # Checkpoint directory. Tensorflow assumes this directory already exists so we need to create it\n",
    "        checkpoint_dir    = os.path.abspath(os.path.join(out_dir, \"checkpoints\"))\n",
    "        checkpoint_prefix = os.path.join(checkpoint_dir, \"model\")\n",
    "        if not os.path.exists(checkpoint_dir):\n",
    "            os.makedirs(checkpoint_dir)\n",
    "        saver = tf.train.Saver()   # removing tf.global_variables()\n",
    "        \n",
    "        # Write vocabulary\n",
    "        vocab_processor.save(os.path.join(out_dir, \"vocab\"))\n",
    "        \n",
    "        \n",
    "        ### NOW start the Training Session\n",
    "        \n",
    "        # init all vars\n",
    "        sess.run(tf.initialize_all_variables())\n",
    "        \n",
    "        # define train step\n",
    "        def train_step(x_batch, y_batch):\n",
    "            ''' A single train step\n",
    "            '''\n",
    "            feed_dict = { \n",
    "                cnn.input_x: x_batch,\n",
    "                cnn.input_y: y_batch,\n",
    "                cnn.dropout_keep_prob: FLAGS.dropout_keep_prob\n",
    "                }\n",
    "            \n",
    "            # one step run\n",
    "            _, step, summaries, loss, accuracy = sess.run(\n",
    "                [train_op, global_step, train_summary_op, cnn.loss, cnn.accuracy], \n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            #print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            train_summary_writer.add_summary(summaries, step)\n",
    "        \n",
    "        # validation/dev step \n",
    "        def dev_step(x_batch, y_batch, writer=None):\n",
    "            ''' Eval model on a validation/dev set\n",
    "            '''\n",
    "            feed_dict = {\n",
    "              cnn.input_x: x_batch,\n",
    "              cnn.input_y: y_batch,\n",
    "              cnn.dropout_keep_prob: 1.0     # no dropout on Test/Validation!\n",
    "            }\n",
    "            step, summaries, loss, accuracy = sess.run(\n",
    "                [global_step, dev_summary_op, cnn.loss, cnn.accuracy],\n",
    "                feed_dict)\n",
    "            time_str = datetime.datetime.now().isoformat()\n",
    "            print(\"{}: step {}, loss {:g}, acc {:g}\".format(time_str, step, loss, accuracy))\n",
    "            if writer:\n",
    "                writer.add_summary(summaries, step)\n",
    "        \n",
    "        ## Generate batches\n",
    "        batches = batch_iter(list(zip(x_train, y_train)),\n",
    "                            FLAGS.batch_size,\n",
    "                            FLAGS.num_epochs)\n",
    "        \n",
    "        # train loop: for each batch\n",
    "        for batch in batches:\n",
    "            x_batch, y_batch = zip(*batch)\n",
    "            # run one step on the batch\n",
    "            train_step(x_batch, y_batch)\n",
    "            current_step = tf.train.global_step(sess, global_step)\n",
    "            \n",
    "            if current_step % FLAGS.evaluate_every == 0:\n",
    "                print(\"\\nEvaluation:\")\n",
    "                dev_step(x_dev, y_dev, writer=dev_summary_writer)\n",
    "                print(\"\")\n",
    "            if current_step % FLAGS.checkpoint_every == 0:\n",
    "                path = saver.save(sess, checkpoint_prefix, global_step=current_step)\n",
    "                print(\"Saved model checkpoint to {}\\n\".format(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Evaluate\n",
    "\n",
    "Let's see how this performs on supplied test data. \n",
    "\n",
    "We will make up some sentences. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "RUN_DIR = 'runs/1481089620'\n",
    "CHECKPOINT_DIR = \"checkpoints\"\n",
    "\n",
    "x_raw = [\"something was off\",\n",
    "        \"it was exciting but would have been better if the movie was 30 minutes shorter\",\n",
    "        \"the acting was so-so but the plot was engaging\",\n",
    "        \"wish i saw this three times\"]\n",
    "y_raw = [0, 0, 1, 1]\n",
    "\n",
    "\n",
    "# map data into vocab\n",
    "vocab_path = os.path.join(RUN_DIR, \".\", \"vocab\" )\n",
    "vocab_processor = learn.preprocessing.VocabularyProcessor.restore(vocab_path)\n",
    "x_test = np.array(list(vocab_processor.transform(x_raw)))\n",
    "\n",
    "#print('x_test shape', x_raw.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Evaluate\n",
    "\n",
    "print(\"\\nEvaluating...\\n\")\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "# Evaluation\n",
    "# ==================================================\n",
    "checkpoint_file = tf.train.latest_checkpoint(RUN_DIR + '/' + CHECKPOINT_DIR)\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    session_conf = tf.ConfigProto(\n",
    "      allow_soft_placement=True,   # FLAGS.allow_soft_placement,\n",
    "      log_device_placement=False)  # FLAGS.log_device_placement)\n",
    "    sess = tf.Session(config=session_conf)\n",
    "    with sess.as_default():\n",
    "        # Load the saved meta graph and restore variables\n",
    "        saver = tf.train.import_meta_graph(\"{}.meta\".format(checkpoint_file))\n",
    "        saver.restore(sess, checkpoint_file)\n",
    "\n",
    "        # Get the placeholders from the graph by name\n",
    "        input_x = graph.get_operation_by_name(\"input_x\").outputs[0]\n",
    "        # input_y = graph.get_operation_by_name(\"input_y\").outputs[0]\n",
    "        dropout_keep_prob = graph.get_operation_by_name(\"dropout_keep_prob\").outputs[0]\n",
    "\n",
    "        # Tensors we want to evaluate\n",
    "        predictions = graph.get_operation_by_name(\"output/predictions\").outputs[0]\n",
    "\n",
    "        # Generate batches for one epoch\n",
    "        batches = data_helpers.batch_iter(list(x_test), BATCH_SIZE, 1, shuffle=False)\n",
    "\n",
    "        # Collect the predictions here\n",
    "        all_predictions = []\n",
    "\n",
    "        for x_test_batch in batches:\n",
    "            batch_predictions = sess.run(predictions, {input_x: x_test_batch, dropout_keep_prob: 1.0})\n",
    "            all_predictions = np.concatenate([all_predictions, batch_predictions])\n",
    "\n",
    "# Print accuracy if y_test is defined\n",
    "if y_test is not None:\n",
    "    correct_predictions = float(sum(all_predictions == y_test))\n",
    "    print(\"Total number of test examples: {}\".format(len(y_test)))\n",
    "    print(\"Accuracy: {:g}\".format(correct_predictions/float(len(y_test))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
